{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_sentence(fn, start_token=\"<s>\", end_token=\"</s>\"):\n",
    "    sents = []\n",
    "    with open(fn) as text:\n",
    "        sent = []\n",
    "        for line in text.readlines():\n",
    "            line = line.strip()\n",
    "            sent.append(line)\n",
    "            if line == end_token:\n",
    "                sents.append(sent)\n",
    "                sent = []\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sentences in source: 45171\n",
      "num sentences in target: 45171\n"
     ]
    }
   ],
   "source": [
    "source = read_in_sentence(\"./train-05/train-source.txt\")\n",
    "target = read_in_sentence(\"./train-05/train-target.txt\")\n",
    "#sanity check..\n",
    "print(f\"num sentences in source: {len(source)}\")\n",
    "print(f\"num sentences in target: {len(target)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 47254 tokens aligned one-to-one out of 48209 tokens\n",
    "- 371 two to one\n",
    "- 369 two to one\n",
    "- 186 two to two\n",
    "- 10 three to two\n",
    "- 38102 unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from itertools import chain, permutations\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Looking at this, it looks like we need to generate \n",
    "# \"up to bigram: s1_t1, s1s2_t1 \"\n",
    "# \"up to trigram_bigram: s1s2_t1t2, s1s2s3_t1t2\"\n",
    "class NGram:\n",
    "    def __init__(self, source: List[List[str]], target: List[List[str]]):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.N = len(self.source)\n",
    "        # alpha smoothing here to prevent divided by 0...\n",
    "        self.target_word_appearance = defaultdict(int)\n",
    "\n",
    "    def generate_ngram(self, n_maps = [(1,1),(2,1),(2,2), (3,2)]):\n",
    "        #pre computed ngram\n",
    "        ngram_dict = defaultdict()\n",
    "        aligns_with_perturb = []\n",
    "        for ind in range(self.N):\n",
    "            ssent, tsent = self.source[ind], self.target[ind]\n",
    "            # FIXME: i am not sure how to align them. We are not given a map. \n",
    "            # We know that source is always longer than target\n",
    "            # but that is about it. So I am just going to align all different possible maps in the sentence...\n",
    "            # Hope there is enough overlap to tell the difference\n",
    "            # Example, I don't know what if target_i maps to source_i-1 source_i. \n",
    "            # And I don't know how many maps are given per sentence...\n",
    "            # so target_i -> source_i, target_i -> source_i source_i+1 as I implement\n",
    "            target_count = set()\n",
    "            for n_gram in n_maps:\n",
    "                nsource, ntarget = n_gram\n",
    "                \n",
    "                target_sent_inplace = [tuple(tsent[i:i+ntarget]) for i in range(len(tsent)-ntarget+1)]\n",
    "                source_sent_inplace = [tuple(ssent[i:i+nsource]) for i in range(len(ssent)-nsource+1)]\n",
    "                if ntarget not in target_count:\n",
    "                    for t in target_sent_inplace:\n",
    "                        self.target_word_appearance[t] += 1\n",
    "                    target_count.add(ntarget)\n",
    "\n",
    "                aligns_with_perturb += [(sword, tar)\n",
    "                                        for tar in target_sent_inplace for sword in source_sent_inplace]\n",
    "            print(f\"{ind}/{self.N}\", end = \"\\r\")\n",
    "        return Counter(aligns_with_perturb) #(sword, starget)\n",
    "            \n",
    "\n",
    "class BiTextWordAlignment:\n",
    "    #implemented as in IBM Model 1 and character tmat as well for unknown words\n",
    "    def __init__(self, source: List[List[str]], target: List[List[str]]):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.fidelity_count, self.fidelity_target_count = None, None\n",
    "        self.fluency, self.wordCountbyN = self.calculate_fluency(self.target) #wordCountbyN is to turned into percentage afterwards\n",
    "        self.fluency_prob = defaultdict(float)\n",
    "\n",
    "    def load_fidelty_stats(self, fidel_count, overall_count):\n",
    "        self.fideltiy_count, self.fidelity_target_count = fidel_count, overall_count\n",
    "        \n",
    "    def calculate_fluency(self, corpus): #That is P(t)\n",
    "        uniword = Counter(chain(*corpus)) #counter |t| = 1\n",
    "        uniwordCount = sum(uniword.values())\n",
    "        biword  = Counter(chain(*map(lambda x: tuple(zip(x, x[1:])), corpus))) #counter for |t| = 2\n",
    "        biwordCount = sum(biword.values())\n",
    "        return Counter({**uniword, **biword}), [uniword,biword]\n",
    "\n",
    "    def _get_fluency_prob(self, tword):\n",
    "        # I precomputed fluency count already\n",
    "        return self.fluency[tword]/self.wordCountbyN[(len(tword)-1)]\n",
    "    \n",
    "    def calculate_word_fertilities(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def train_lexical_prob(self, lexicalP, n_maps=[(1, 1), (2, 1), (2, 2), (3, 2)]):\n",
    "        \"\"\"modified from instructor's code to align for different scenarios:\n",
    "        https://colab.research.google.com/drive/1V6ZwTsBe2s7tAmHTkYqpENrzt4zOrnu3?usp=sharing#scrollTo=NLmGPFha8982\n",
    "        \"\"\"\n",
    "        \n",
    "        total = defaultdict(float)  # keys are source language words\n",
    "\n",
    "        for ssent, tsent in zip(self.source, self.target):\n",
    "            sent_totals = defaultdict(float)\n",
    "            for sword in ssent:\n",
    "                for tword in tsent:\n",
    "                    sent_totals[sword] += lexicalP[(sword, tword)]\n",
    "            for s in ssent:\n",
    "                for t in tsent:\n",
    "                    self.fidelity_count[(s, t)] += lexicalP[(s, t)]/sent_totals[s]\n",
    "                    total[t] += lexicalP[(s, t)]/sent_totals[s]\n",
    "        for t in tv:\n",
    "            for s in sv:\n",
    "                lexicalP[(s, t)] = self.fidelity_count[(s, t)]/total[t]\n",
    "\n",
    "    def align_chars(self):\n",
    "        pass\n",
    "class HMM:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def train(self, fluency, fidelity):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All training data here...\n",
    "#import pickle\n",
    "#ngram_train = NGram(source, target)\n",
    "#ngram_train.generate_ngram()\n",
    "#with open('train-05/ngram_count.train', 'wb') as outputfile:\n",
    "#    pickle.dump(ngram_count, outputfile)\n",
    "#with open('train-05/ngram_count_target.train', 'wb') as out:\n",
    "#    pickle.dump(ngram_train.target_word_appearance, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#btwa = BiTextWordAlignment(source, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluent = btwa.fluency\n",
    "sum(fluent.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{i: (v/sum(fluent.values())) for i, v in fluent.items()}\n",
    "#%timeit\n",
    "#dict(map(lambda x: (x[0], x[1]/sum(fluent.values())), dict(fluent).items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "tsent, ssent = [1, 2, 3, 4], [1, 2, 3,4]\n",
    "ntarget, nsource = 2,3\n",
    "target_sent_inplace = [tuple(tsent[i:i+ntarget])\n",
    "                       for i in range(len(tsent)-ntarget+1)]\n",
    "source_sent_inplace = [tuple(ssent[i:i+nsource])\n",
    "                       for i in range(len(ssent)-nsource+1)]\n",
    "\n",
    "[(sword, tar) for tar in target_sent_inplace for sword in source_sent_inplace]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
