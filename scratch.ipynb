{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_sentence(fn, start_token=\"<s>\", end_token=\"</s>\"):\n",
    "    sents = []\n",
    "    with open(fn) as text:\n",
    "        sent = []\n",
    "        for line in text.readlines():\n",
    "            line = line.strip()\n",
    "            sent.append(line)\n",
    "            if line == end_token:\n",
    "                sents.append(sent)\n",
    "                sent = []\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sentences in source: 45171\n",
      "num sentences in target: 45171\n"
     ]
    }
   ],
   "source": [
    "source = read_in_sentence(\"./train-05/train-source.txt\")\n",
    "target = read_in_sentence(\"./train-05/train-target.txt\")\n",
    "#sanity check..\n",
    "print(f\"num sentences in source: {len(source)}\")\n",
    "print(f\"num sentences in target: {len(target)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 47254 tokens aligned one-to-one out of 48209 tokens\n",
    "- 371 two to one\n",
    "- 369 two to one\n",
    "- 186 two to two\n",
    "- 10 three to two\n",
    "- 38102 unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from itertools import chain, permutations\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Looking at this, it looks like we need to generate \n",
    "# \"up to bigram: s1_t1, s1s2_t1 \"\n",
    "# \"up to trigram_bigram: s1s2_t1t2, s1s2s3_t1t2\"\n",
    "class NGram:\n",
    "    def __init__(self, source: List[List[str]], target: List[List[str]]):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.N = len(self.source)\n",
    "        self.target_word_appearance = defaultdict(int)\n",
    "        self.source_word_appearance = defaultdict(int)\n",
    "\n",
    "    def generate_ngram(self, n_maps = [(1,1),(2,1),(2,2), (3,2)]):\n",
    "        #pre computed ngram\n",
    "        ngram_dict = defaultdict()\n",
    "        aligns_with_perturb = []\n",
    "        for ind in range(self.N):\n",
    "            ssent, tsent = self.source[ind], self.target[ind]\n",
    "            # FIXME: i am not sure how to align them. We are not given a map. \n",
    "            # We know that source is always longer than target\n",
    "            # but that is about it. So I am just going to align all different possible maps in the sentence...\n",
    "            # Hope there is enough overlap to tell the difference\n",
    "            # Example, I don't know what if target_i maps to source_i-1 source_i. \n",
    "            # And I don't know how many maps are given per sentence...\n",
    "            # so target_i -> source_i, target_i -> source_i source_i+1 as I implement\n",
    "            target_count = set()\n",
    "            source_count = set()\n",
    "            for n_gram in n_maps:\n",
    "                nsource, ntarget = n_gram\n",
    "                \n",
    "                target_sent_inplace = [tuple(tsent[i:i+ntarget]) for i in range(len(tsent)-ntarget+1)]\n",
    "                source_sent_inplace = [tuple(ssent[i:i+nsource]) for i in range(len(ssent)-nsource+1)]\n",
    "                if ntarget not in target_count:\n",
    "                    for t in target_sent_inplace:\n",
    "                        self.target_word_appearance[t] += 1\n",
    "                    target_count.add(ntarget)\n",
    "                if nsource not in source_count:\n",
    "                    for s in source_sent_inplace:\n",
    "                        self.source_word_appearance[s] += 1\n",
    "                    source_count.add(nsource)\n",
    "\n",
    "                aligns_with_perturb += [(sword, tar)\n",
    "                                        for tar in target_sent_inplace for sword in source_sent_inplace]\n",
    "            print(f\"{ind}/{self.N}\", end = \"\\r\")\n",
    "        return Counter(aligns_with_perturb) #(sword, starget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All training data saved here...\n",
    "#import pickle\n",
    "#ngram_train = NGram(source, target)\n",
    "#ngram_train.generate_ngram()\n",
    "#with open('train-05/ngram_count.train', 'wb') as outputfile:\n",
    "#    pickle.dump(ngram_count, outputfile)\n",
    "#with open('train-05/ngram_count_target.train', 'wb') as out:\n",
    "#    pickle.dump(ngram_train.target_word_appearance, out)\n",
    "#with open('train-05/ngram_count_source.train', 'wb') as out:\n",
    "#    pickle.dump(ngram_train.source_word_appearance, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BiTextWordAlignment:\n",
    "    #implemented as in IBM Model 1 and character tmat as well for unknown words\n",
    "    def __init__(self, source: List[List[str]], target: List[List[str]]):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.fidelity_count, self.fidelity_target_count, self.fidelity_source_count = None, None, None\n",
    "        self.wordCountbyN = defaultdict(int)\n",
    "        self.fluency = self.calculate_fluency(self.target) #wordCountbyN is to turned into percentage afterwards\n",
    "        self.fluency_prob = defaultdict(float)\n",
    "\n",
    "    def load_fidelty_stats(self, fidel_count, overall_count_target, overall_count_source):\n",
    "        self.fidelity_count, self.fidelity_target_count, self.fidelity_source_count = fidel_count, overall_count_target, overall_count_source\n",
    "\n",
    "    def _get_fidelity_prob(self, sword: tuple, tword: tuple):\n",
    "        return self.fidelity_count[(sword, tword)]/self.fidelity_target_count[tword]\n",
    "        \n",
    "    def calculate_fluency(self, corpus): #That is P(t)\n",
    "        uniword = Counter(chain(*map(lambda x: tuple(x), corpus))) #counter |t| = 1\n",
    "        self.wordCountbyN[1] = sum(uniword.values())\n",
    "        biword  = Counter(chain(*map(lambda x: tuple(zip(x, x[1:])), corpus))) #counter for |t| = 2\n",
    "        self.wordCountbyN[2] = sum(biword.values())\n",
    "        return Counter({**uniword, **biword})\n",
    "\n",
    "    def _get_fluency_prob(self, tword: tuple):\n",
    "        # I precomputed fluency count already\n",
    "        return self.fluency[tword]/self.wordCountbyN[len(tword)]\n",
    "\n",
    "    def train_lexical_prob(self, lexicalP, n_maps=[(1, 1), (2, 1), (2, 2), (3, 2)]):\n",
    "        \"\"\"modified from instructor's code to align for different scenarios:\n",
    "        https://colab.research.google.com/drive/1V6ZwTsBe2s7tAmHTkYqpENrzt4zOrnu3?usp=sharing#scrollTo=NLmGPFha8982\n",
    "        \"\"\"\n",
    "        \n",
    "        total = defaultdict(float)  # keys are source language words\n",
    "        C = defaultdict(float)\n",
    "        \n",
    "        for ssent, tsent in zip(self.source, self.target):\n",
    "            for n_gram in n_maps: #all possible mappings\n",
    "                sent_totals = defaultdict(float)\n",
    "                nsource, ntarget = n_gram\n",
    "                target_sent_inplace = [tuple(tsent[i:i+ntarget])\n",
    "                                    for i in range(len(tsent)-ntarget+1)]\n",
    "                source_sent_inplace = [\n",
    "                    tuple(ssent[i:i+nsource]) for i in range(len(ssent)-nsource+1)]\n",
    "\n",
    "                for sword in source_sent_inplace:  # source_sent_inplace\n",
    "                    for tword in target_sent_inplace:\n",
    "                        sent_totals[sword] += lexicalP[(sword, tword)]\n",
    "                for sword in source_sent_inplace:  # source_sent_inplace\n",
    "                    for tword in target_sent_inplace:\n",
    "                        C[(sword, tword)] += lexicalP[(sword, tword)]/sent_totals[sword]\n",
    "                        total[tword] += lexicalP[(sword, tword)]/sent_totals[sword]\n",
    "        for tword in self.fidelity_target_count:\n",
    "            for sword in self.fidelity_source_count:\n",
    "                lexicalP[(sword, tword)] = C[(sword, tword)] / total[tword]\n",
    "\n",
    "    def align_chars(self):\n",
    "        pass\n",
    "class HMMChar: #for unknown words\n",
    "    def __init__(self):\n",
    "        self.emission = defaultdict(lambda: defaultdict(int)) #emission[sword_i][tword_i]\n",
    "        self.count_char_tar = defaultdict(int) #[tword_i]\n",
    "        self.all_char_count = 0\n",
    "        self.all_chars = set()\n",
    "        self.tmat = defaultdict(lambda: defaultdict(int)) #tmat[tword_i][tword_i+1]\n",
    "\n",
    "    def leveinstein(self, sword:str, tword:str):\n",
    "        ###target###\n",
    "        #\n",
    "        #src\n",
    "        #\n",
    "        #sum(replacement + insertions + deletions)\n",
    "        m = len(sword)+1 # empty string as well\n",
    "        n = len(tword)+1 # empty string as well\n",
    "        dp_mat = [[0 for i in range(n)] for j in range(m)]\n",
    "        #empty string vs target_substring\n",
    "        for index in range(0, m):\n",
    "            dp_mat[index][0] = index\n",
    "        # empty string vs source_substring\n",
    "        dp_mat[0] = list(range(0,n))\n",
    "\n",
    "        for t in range(1, n) :\n",
    "            for s in range(1, m):\n",
    "                if sword[s-1] == tword[t-1]: #previous step\n",
    "                    diff = 0\n",
    "                else:\n",
    "                    diff = 1\n",
    "                dp_mat[s][t] = min(dp_mat[s-1][t-1] + diff , # replacement\n",
    "                                    dp_mat[s][t-1] + 1, #deletion\n",
    "                                    dp_mat[s-1][t] + 1 ) #insetion\n",
    "            \n",
    "        return dp_mat\n",
    "    def decode(self, sword:str, tword:str):\n",
    "        dp_mat = self.leveinstein(sword, tword)\n",
    "        i,j = len(sword), len(tword)\n",
    "        sedit = []\n",
    "        tedit = []\n",
    "        while (i != 0 and j != 0):\n",
    "            prev_distance = min(dp_mat[i-1][j-1], dp_mat[i][j-1], dp_mat[i-1][j])\n",
    "\n",
    "            if ( prev_distance == dp_mat[i][j]):  # match\n",
    "                #print(\"M\")\n",
    "                i,j = i-1,j-1\n",
    "                sedit.append(sword[i])\n",
    "                tedit.append(tword[j])\n",
    "            elif(j and i and prev_distance == dp_mat[i-1][j-1] ): #replacement, coming from diagonal\n",
    "                #print(\"R\")\n",
    "                i, j = i-1, j-1\n",
    "                sedit.append(sword[i])\n",
    "                tedit.append(tword[j])\n",
    "            elif(i and prev_distance == dp_mat[i-1][j] ): #deletion\n",
    "                #print(\"D\")\n",
    "                i, j = i-1, j\n",
    "                sedit.append(sword[i])\n",
    "                tedit.append(\"-\")\n",
    "                \n",
    "            elif(j and prev_distance == dp_mat[i][j-1]): #insertion\n",
    "                #print(\"I\")\n",
    "                i, j = i, j-1\n",
    "                sedit.append(\"-\")\n",
    "                tedit.append(tword[j])\n",
    "        return sedit[::-1], tedit[::-1]\n",
    "    def make_emission(self, src_tar_list: List[tuple]):\n",
    "        i = 0\n",
    "        for tup in src_tar_list:\n",
    "            i+=1\n",
    "            sedit, tedit = self.decode(tup[0], tup[1])\n",
    "            tedit_lagged = tedit[1:]\n",
    "            for schar, tchar in zip(sedit, tedit):\n",
    "                self.emission[schar][tchar] += 1\n",
    "                self.count_char_tar[tchar] += 1\n",
    "                self.all_char_count += 1\n",
    "                self.all_chars.add(tchar)\n",
    "                if len(tedit_lagged) >= 1:\n",
    "                    self.tmat[schar][tedit_lagged[0]]\n",
    "                    tedit_lagged=tedit_lagged[1:]\n",
    "            if (i >= 1000000):\n",
    "                break\n",
    "            print(f\"{i}/{len(src_tar_list)}\", end=\"\\r\")\n",
    "\n",
    "    def get_emission_prob(self, src, tag):\n",
    "        return self.emission[src][tag]/self.count_char_tar[tag]\n",
    "    \n",
    "    \n",
    "    def get_transition_prob(self, tag_prev, tag_cur):\n",
    "        return self.tmat[tag_prev][tag_cur]/self.count_char_tar[tag_cur]\n",
    "    \n",
    "    def get_char_tar_prob(self, tag):\n",
    "        return self.count_char_tar[tag]/self.all_char_count\n",
    "\n",
    "    def viterbi(self, srcword:str):\n",
    "        V = defaultdict(lambda : defaultdict(float)) #[src][tar]\n",
    "        B = defaultdict(lambda : defaultdict(str)) #backpointer\n",
    "\n",
    "        for char in self.all_chars:\n",
    "            V[0][char] = self.get_char_tar_prob(char) * self.get_emission_prob(srcword[0], char) #src -> tar\n",
    "        for i in range(1,len(srcword)):\n",
    "            for char in self.all_chars:\n",
    "                pair = self.argmax(V,char,i)\n",
    "                B[i][char] = pair[0]\n",
    "                V[i][char] = pair[1]*self.get_emission_prob(srcword[i], char)\n",
    "            #print(V[i])\n",
    "        \n",
    "\n",
    "        final_labels = self.get_best_tag(srcword, V, B)\n",
    "        return final_labels,V\n",
    "\n",
    "    def argmax(self, V,tchar,i):\n",
    "        ans=-1\n",
    "        best=None\n",
    "        for s in self.all_chars:\n",
    "            temp = V[i-1][s] + self.get_transition_prob(tchar,s) #target_i to target_i+1\n",
    "            if temp > ans:\n",
    "                ans = temp\n",
    "                best = s\n",
    "        return (best,ans)\n",
    "\n",
    "    def get_best_tag(self, sent, V,B):\n",
    "        best_ending = None\n",
    "        best_max = -1\n",
    "\n",
    "        for tag in self.all_chars:\n",
    "            if V[len(sent) - 1][tag] > best_max:\n",
    "                best_max = V[len(sent) - 1][tag]\n",
    "                best_ending = tag\n",
    "        seq = [best_ending]\n",
    "        for i in reversed(range(1, len(sent))):\n",
    "            prev = B[i][seq[-1]]\n",
    "            #print( seq[-1])\n",
    "            seq.append(prev)\n",
    "        #print(len(seq), len(sent))\n",
    "        return seq[::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "btwa = BiTextWordAlignment(source, target)\n",
    "fidel_count_train = pickle.load(open(\"train-05/ngram_count.train\",\"rb\"))\n",
    "overall_count_target = pickle.load(open(\"train-05/ngram_count_target.train\",\"rb\"))\n",
    "overall_count_source = pickle.load(open(\"train-05/ngram_count_source.train\",\"rb\"))\n",
    "btwa.load_fidelty_stats(fidel_count_train,overall_count_target, overall_count_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalizing lexProb\n",
    "lexicalP = defaultdict(lambda: defaultdict(float))\n",
    "for tup in fidel_count_train:\n",
    "    s,t = tup\n",
    "    lexicalP[s][t] = btwa._get_fidelity_prob(s,t)\n",
    "\n",
    "#take too long even for 1 iteration and rightfully so with the way I set it up\n",
    "# lets just use lexicalP here from pure count here... So no EM :< \n",
    "#btwa.train_lexical_prob(lexicalP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999999/50616099\r"
     ]
    }
   ],
   "source": [
    "hmm = HMMChar()\n",
    "hmm.make_emission([(\" \".join(tup[0]), \" \".join(tup[1])) for tup in fidel_count_train]) #50 million yikes!.I implemented early stopping cause no point to run quaradtic on 50 millions\n",
    "#hmm.viterbi(\"cba\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For decoding, I will do the greedy cause of my bad setup\n",
    "#given lexical probability, we just align them to the one with highest probability (fidelity *fluency)\n",
    "#not very generalizable here...\n",
    "#source s1, s1s2, s1s2s3\n",
    "def predict_from_scratch(sSent: List, lexicalP):\n",
    "    sTargetPredicted = [\"<s>\"]\n",
    "    #if word is in here. let's see if s1s2 and s1s2s3 is there as well\n",
    "    #print(len(sSent))\n",
    "    for ind, s1 in enumerate(sSent[1:-1]):\n",
    "\n",
    "        bestTarget, bestSrc = None, None\n",
    "        bestScore = -100000\n",
    "        #s1, s1s2, s1s2s3 = (s1,), None, None\n",
    "        #print(s1)\n",
    "        s1 = (s1,)\n",
    "        #print(ind, s1)\n",
    "        if (s1 in lexicalP):\n",
    "            possible_aligns1 = set(lexicalP[s1].keys())\n",
    "            #print(len(possible_aligns1))\n",
    "            for align in possible_aligns1:\n",
    "                src, tar = s1, align\n",
    "                prod = lexicalP[src][tar]\n",
    "                if  \"<s>\" in tar or \"<s>\" in tar or len(tar) >= 2:\n",
    "                    continue\n",
    "                if prod > bestScore:\n",
    "                    bestScore = prod\n",
    "                    bestTarget = tar\n",
    "                    bestSrc = src\n",
    "            #print(bestScore)\n",
    "            #print(s1, bestSrc, bestTarget, bestScore)\n",
    "            sTargetPredicted+= list(bestTarget)\n",
    "            \n",
    "        else:\n",
    "            #never seen before so HMM at character level\n",
    "            hmm_translated,_ = hmm.viterbi(s1[0])\n",
    "            #print(\"HMM\", hmm_translated)\n",
    "            sTargetPredicted.append(\"\".join(i for i in hmm_translated if i != \"-\")) #\"-\" is epsilon\n",
    "    return sTargetPredicted + [\"</s>\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sourceTest = read_in_sentence(\"test-05/test-source.txt\")\n",
    "targetTest = read_in_sentence(\"test-05/test-target.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Faighte',\n",
       " 'síbhruinneall',\n",
       " 'Freeman',\n",
       " 'saighid',\n",
       " 'margaí',\n",
       " 'Pobal',\n",
       " 'hearbhr4thair',\n",
       " 'luibhearnaí',\n",
       " 'cailíseacha',\n",
       " 'Dúcharraige',\n",
       " '</s>']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_from_scratch(sourceTest[0], lexicalP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000\r"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "i = 0\n",
    "for sent in sourceTest:\n",
    "    i+=1\n",
    "    predicted.append(predict_from_scratch(sent, lexicalP))\n",
    "    print(f\"{i}/{len(sourceTest)}\", end = \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.115962876732479e-155"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "corpus_bleu(predicted, targetTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
