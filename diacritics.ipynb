{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBTHMeuQyWtZ"
      },
      "source": [
        "Data are found in ```curl --remote-name-all https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2607{/cs.zip,/es.zip,/fr.zip,/ga.zip,/hr.zip,/hu.zip,/lv.zip,/pl.zip,/ro.zip,/sk.zip,/tr.zip,/vi.zip,/stripping_diacritics.zip}```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOfzDNe6yWtd"
      },
      "source": [
        "Data also contain some mixed and borrowed words. Some put diacritics on them to make those phonetic, but not always. Test data also contain some other languages (Mandarin and English) perhaps to make sure the system is smart enough to not restore diacritics for these"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C5TmwyWyWte"
      },
      "outputs": [],
      "source": [
        "#!curl --remote-name-all https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-2607{/vi.zip}\n",
        "#!unzip vi.zip\n",
        "#!cd vi && xz -v -d *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XlFUe9A0yWtf"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def remove_accents(input_str): #this removes some special characters which we dont like\n",
        "    input_str = re.sub(r'đ', 'd', input_str)\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    ascii_form = nfkd_form.encode('ascii','ignore')\n",
        "    text = ascii_form.decode(\"utf-8\")\n",
        "    return str(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5UuyPLTkyWth"
      },
      "outputs": [],
      "source": [
        "charToDiacritized = defaultdict(lambda: defaultdict(list))\n",
        "charToDiacritized[\"a\"][0] = [\"a\", \"à\", \"á\", \"ả\", \"ã\", \"ạ\"]\n",
        "charToDiacritized[\"a\"][1] = [\"ă\", \"ằ\", \"ắ\", \"ẳ\", \"ẵ\", \"ặ\"]\n",
        "charToDiacritized[\"a\"][2] = [\"â\", \"ầ\", \"ấ\", \"ẩ\", \"ẫ\", \"ậ\"]\n",
        "charToDiacritized[\"e\"][0] = [\"e\", \"è\", \"é\", \"ẻ\", \"ẽ\", \"ẹ\"]\n",
        "charToDiacritized[\"e\"][1] = [\"ê\", \"ề\", \"ế\", \"ể\", \"ễ\", \"ệ\"]\n",
        "charToDiacritized[\"i\"][0] = [\"i\", \"ì\", \"í\", \"ỉ\", \"ĩ\", \"ị\"]\n",
        "charToDiacritized[\"o\"][0] = [\"o\", \"ò\", \"ó\", \"ỏ\", \"õ\", \"ọ\"]\n",
        "charToDiacritized[\"o\"][1] = [\"ô\", \"ồ\", \"ố\", \"ổ\", \"ỗ\", \"ộ\"]\n",
        "charToDiacritized[\"o\"][2] = [\"ơ\", \"ờ\", \"ớ\", \"ở\", \"ỡ\", \"ợ\"]\n",
        "charToDiacritized[\"u\"][0] = [\"u\", \"ù\", \"ú\", \"ủ\", \"ũ\", \"ụ\"]\n",
        "charToDiacritized[\"u\"][1] = [\"ư\", \"ừ\", \"ứ\", \"ử\", \"ữ\", \"ự\"]\n",
        "charToDiacritized[\"y\"][0] = [\"y\", \"ỳ\", \"ý\", \"ỷ\", \"ỹ\", \"ỵ\"]\n",
        "charToDiacritized[\"d\"][0] = [\"đ\"]\n",
        "diacritizedToFeature = defaultdict(tuple)\n",
        "for root in charToDiacritized:\n",
        "    for diacr_type in charToDiacritized[root]:\n",
        "        for idx, char in enumerate(charToDiacritized[root][diacr_type]):\n",
        "            diacritizedToFeature[char] = (root, diacr_type, idx % 6)\n",
        "            diacritizedToFeature[char.upper()] = (root.upper(), diacr_type, idx % 6)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Re1mUC7JyWti"
      },
      "outputs": [],
      "source": [
        "def featurize_vi_diacritics(input_str: str):\n",
        "    #train (char_root, diacritic_type, tone )\n",
        "    # by diacritic type, i mean like non-tonal...\n",
        "    # a: 0 base(a), 1 aw, 2 aa\n",
        "    # e: 0 base (e), 1 ee\n",
        "    # o: 0 base (o), 1 oo, 2 ow\n",
        "    # u: 0 base (u), 1 uw\n",
        "    # d: 0 base (d), 1 dd\n",
        "    # tonal\n",
        "    #0 = nothing, 1 = grave, 2 = acute, 3 = hook, 4 = tilde, 5 = dot\n",
        "    input_list = []\n",
        "    for i in input_str:\n",
        "        if i in diacritizedToFeature: #special diacritized character\n",
        "            input_list.append(diacritizedToFeature[i][1]*10 + diacritizedToFeature[i][2]) #i dont think it is necessaary to index diacritics...\n",
        "        else:\n",
        "            input_list.append(0)\n",
        "    return input_list\n",
        "\n",
        "\n",
        "def remove_vi_diacritics(input_str: str):\n",
        "    input_list = []\n",
        "    for i in input_str:\n",
        "        if i in diacritizedToFeature:\n",
        "            input_list.append(diacritizedToFeature[i][0])\n",
        "        else:\n",
        "            input_list.append(i)\n",
        "    return input_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PQb2cDGRyWtj"
      },
      "outputs": [],
      "source": [
        "allX = []\n",
        "allY = []\n",
        "vocabs = {\"<S>\", \"</S>\"}\n",
        "vocabsY = set()\n",
        "for lines in open(\"target_train.txt\").readlines():\n",
        "    sentX = [\"<S>\"] + remove_vi_diacritics(lines.strip()) + [\"</S>\"]\n",
        "    sentY = [0] + featurize_vi_diacritics(lines.strip()) + [0]\n",
        "    allX.append(sentX)\n",
        "    allY.append(sentY)\n",
        "    vocabsY.update(set(i for i in sentY))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "trainX, testX, trainY, testY = train_test_split(allX, allY, test_size=0.2)"
      ],
      "metadata": {
        "id": "P4kRghJTs0cs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabs.update(set(i for xSent in trainX for i in xSent) )"
      ],
      "metadata": {
        "id": "3QVlFGfgthva"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KuHcN2WQyWtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02bfa670-0d6f-45fd-c631-0da9b77b4663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<S>', 'c', 'o', ' ', 't', 'h', 'e', ' ', 'n', 'o', 'i', ' ', 'n', 'h', 'u', ' ', 'v', 'a', 'y', ' ', 't', 'r', 'a', 'n', 'h', ' ', 'c', 'h', 'a', 'p', ' ', 'l', 'a', 'n', 'h', ' ', 't', 'h', 'o', ' ', 'g', 'i', 'u', 'a', ' ', 'v', 'i', 'e', 't', ' ', 'n', 'a', 'm', ' ', 'v', 'a', ' ', 't', 'r', 'u', 'n', 'g', ' ', 'q', 'u', 'o', 'c', ' ', 'd', 'a', ' ', 'a', 'm', ' ', 'i', ' ', 'd', 'i', 'e', 'n', ' ', 'r', 'a', ' ', 't', 'u', ' ', 'n', 'a', 'm', ' ', '1', '9', '7', '4', ' ', '.', '</S>']\n",
            "[0, 0, 2, 0, 0, 0, 13, 0, 0, 2, 0, 0, 0, 0, 10, 0, 0, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 0, 0, 0, 4, 0, 0, 0, 0, 0, 13, 0, 0, 0, 14, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 4, 0, 20, 0, 0, 3, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 11, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(trainX[0])\n",
        "print(trainY[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "li0hHnlMyWtl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4804c1e6-3f0e-457b-d482-c3c73e2d253a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "655934"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(trainX)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunkify(x, sentWindow = 60):\n",
        "  return [x[i:i+60] for i in range(0, len(x), sentWindow)]"
      ],
      "metadata": {
        "id": "UEnyXiQ0L38k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX_chunked = []\n",
        "for i in trainX:\n",
        "  trainX_chunked += chunkify(i)\n"
      ],
      "metadata": {
        "id": "KynKJ3vcONyK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainY_chunked = []\n",
        "for i in trainY:\n",
        "  trainY_chunked += chunkify(i)\n"
      ],
      "metadata": {
        "id": "NPMJzg7VRvRr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7arpU8NwyWtl"
      },
      "outputs": [],
      "source": [
        "vocabs = set(filter(lambda x: x.isascii() , vocabs)) #dataset has Mandarin characters, i don't think we need to. Most of them the system should learn that it is unchanged anyways...\n",
        "vocabs = list(vocabs)\n",
        "vocabs.append(\"<PAD>\")\n",
        "vocabs.append(\"<UNK>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zfY8JtfyyWtm"
      },
      "outputs": [],
      "source": [
        "char2index = {w: i for i,w in enumerate(vocabs)}\n",
        "X_train_index = [[char2index[char] if char.isascii() else char2index[\"<UNK>\"] for char in sent] for sent in trainX_chunked]\n",
        "vocabsY = list(vocabsY)\n",
        "tag2index = {v:i for i,v in enumerate(vocabsY)}\n",
        "Y_train_index = [[tag2index[char] for char in sent] for sent in trainY_chunked]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P-dsQ744yWtm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "CHAR_VOCABS = len(vocabs)\n",
        "CHAR_EMBEDDING = 50\n",
        "CHAR_MAX_LEN = 60  # longest sentence\n",
        "\n",
        "X_padded = pad_sequences(\n",
        "    maxlen=CHAR_MAX_LEN, sequences=X_train_index, padding=\"post\", value=char2index[\"<PAD>\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_padded = pad_sequences(\n",
        "    maxlen=CHAR_MAX_LEN, sequences=Y_train_index, padding=\"post\", value=0)"
      ],
      "metadata": {
        "id": "aafd7PEiHbgP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_padded = np.array(Y_padded)"
      ],
      "metadata": {
        "id": "adIXSCsZkD4_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "Y_padded = [to_categorical(i, num_classes=len(vocabsY)) for i in Y_padded]"
      ],
      "metadata": {
        "id": "XOi7y0sXdJZL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "v56cD1MjyWtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2192d6b-9181-4579-cf7b-fecef3fc499c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 60, 50)            3400      \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 60, 100)          40400     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 60, 18)           1818      \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 45,618\n",
            "Trainable params: 45,618\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed\n",
        "\n",
        "character_model = Sequential()\n",
        "character_model.add(Embedding(input_dim = CHAR_VOCABS, output_dim = CHAR_EMBEDDING, input_length = CHAR_MAX_LEN) )\n",
        "character_model.add(Bidirectional(LSTM(units = 50, return_sequences = True, recurrent_dropout = 0.1)) )\n",
        "#character_model.add(LSTM(100))\n",
        "character_model.add(TimeDistributed(Dense(18, activation='softmax')))\n",
        "\n",
        "character_model.compile(\"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "character_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Y_padded3D = Y_padded.reshape(*Y_padded.shape, 1)"
      ],
      "metadata": {
        "id": "_C5HZcvKm45k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "L5M5KBhKyWto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e5dfa2-ba49-4982-9931-70c4e4dc3db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "137/137 [==============================] - ETA: 0s - loss: 0.6477 - accuracy: 0.8647\n",
            "Epoch 1: saving model to character_model.ckpt\n",
            "137/137 [==============================] - 367s 3s/step - loss: 0.6477 - accuracy: 0.8647 - val_loss: 0.5140 - val_accuracy: 0.8647\n",
            "Epoch 2/10\n",
            "137/137 [==============================] - ETA: 0s - loss: 0.4937 - accuracy: 0.8646\n",
            "Epoch 2: saving model to character_model.ckpt\n",
            "137/137 [==============================] - 373s 3s/step - loss: 0.4937 - accuracy: 0.8646 - val_loss: 0.4716 - val_accuracy: 0.8644\n",
            "Epoch 3/10\n",
            "137/137 [==============================] - ETA: 0s - loss: 0.4447 - accuracy: 0.8654\n",
            "Epoch 3: saving model to character_model.ckpt\n",
            "137/137 [==============================] - 307s 2s/step - loss: 0.4447 - accuracy: 0.8654 - val_loss: 0.4167 - val_accuracy: 0.8722\n",
            "Epoch 4/10\n",
            "137/137 [==============================] - ETA: 0s - loss: 0.3927 - accuracy: 0.8790\n",
            "Epoch 4: saving model to character_model.ckpt\n",
            "137/137 [==============================] - 299s 2s/step - loss: 0.3927 - accuracy: 0.8790 - val_loss: 0.3655 - val_accuracy: 0.8884\n",
            "Epoch 5/10\n",
            "137/137 [==============================] - ETA: 0s - loss: 0.3511 - accuracy: 0.8918\n",
            "Epoch 5: saving model to character_model.ckpt\n",
            "137/137 [==============================] - 307s 2s/step - loss: 0.3511 - accuracy: 0.8918 - val_loss: 0.3337 - val_accuracy: 0.8971\n",
            "Epoch 6/10\n",
            "137/137 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.8981\n",
            "Epoch 6: saving model to character_model.ckpt\n",
            "137/137 [==============================] - 303s 2s/step - loss: 0.3253 - accuracy: 0.8981 - val_loss: 0.3116 - val_accuracy: 0.9014\n",
            "Epoch 7/10\n",
            "137/137 [==============================] - ETA: 0s - loss: 0.3063 - accuracy: 0.9027\n",
            "Epoch 7: saving model to character_model.ckpt\n",
            "137/137 [==============================] - 313s 2s/step - loss: 0.3063 - accuracy: 0.9027 - val_loss: 0.2943 - val_accuracy: 0.9061\n",
            "Epoch 8/10\n",
            "137/137 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.9065\n",
            "Epoch 8: saving model to character_model.ckpt\n",
            "137/137 [==============================] - 324s 2s/step - loss: 0.2914 - accuracy: 0.9065 - val_loss: 0.2807 - val_accuracy: 0.9095\n",
            "Epoch 9/10\n",
            "137/137 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.9092\n",
            "Epoch 9: saving model to character_model.ckpt\n",
            "137/137 [==============================] - 321s 2s/step - loss: 0.2792 - accuracy: 0.9092 - val_loss: 0.2691 - val_accuracy: 0.9124\n",
            "Epoch 10/10\n",
            "137/137 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.9118\n",
            "Epoch 10: saving model to character_model.ckpt\n",
            "137/137 [==============================] - 311s 2s/step - loss: 0.2686 - accuracy: 0.9118 - val_loss: 0.2589 - val_accuracy: 0.9147\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa0a4db8790>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "checkpoint_path = \"character_model.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "character_model.fit(X_padded, Y_padded, batch_size = 10000, epochs=10, validation_split=0.2, verbose = True, callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction Time"
      ],
      "metadata": {
        "id": "0eiho6Ps_5Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_index = [[char2index[char] if char.isascii() else char2index[\"<UNK>\"] for char in sent] for sent in testX]\n",
        "testX_chunked = []\n",
        "for i in X_test_index:\n",
        "  testX_chunked += chunkify(i)\n",
        "X_test_padded = pad_sequences(\n",
        "    maxlen=CHAR_MAX_LEN, sequences=testX_chunked, padding=\"post\", value=char2index[\"<PAD>\"])"
      ],
      "metadata": {
        "id": "QK0dF4Np-Qsn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_X_test = character_model.predict(X_test_padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18SP6TGa_JXz",
        "outputId": "ac76211e-ba65-4245-e143-5a752cbf88dd"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13276/13276 [==============================] - 197s 15ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#greedy decoding\n",
        "def greedy_decoder(data):\n",
        "  return [np.argmax(s, 1) for s in data]"
      ],
      "metadata": {
        "id": "CAdQ4CzaBV2A"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(softmax_X_test[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-Tov4wgDOd7",
        "outputId": "fe4da8fc-0125-452b-f3af-adabde0b06c3"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_Xtest = greedy_decoder(softmax_X_test)"
      ],
      "metadata": {
        "id": "RXlWfqiyQhL-"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allX = []\n",
        "allY = []\n",
        "sent = []\n",
        "predicted = []\n",
        "start = True\n",
        "for ci, chunk in enumerate(X_test_padded):\n",
        "  for ji, j in enumerate(chunk):\n",
        "    if j == char2index[\"<S>\"]:\n",
        "      sent.append(X_test_padded[ci][ji])\n",
        "      predicted.append(decoded_Xtest[ci][ji])\n",
        "      start = True\n",
        "    elif j == char2index[\"</S>\"]:\n",
        "      sent.append(X_test_padded[ci][ji])\n",
        "      predicted.append(decoded_Xtest[ci][ji])\n",
        "      start = False\n",
        "      allX.append(sent)\n",
        "      allY.append(predicted)\n",
        "      sent = []\n",
        "      predicted = []\n",
        "    elif not start:\n",
        "      continue\n",
        "    elif start:\n",
        "      sent.append(X_test_padded[ci][ji])\n",
        "      predicted.append(decoded_Xtest[ci][ji])\n",
        "    \n"
      ],
      "metadata": {
        "id": "1866aBvQQxGO"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2char = {i: v for i,v in char2index.items() }"
      ],
      "metadata": {
        "id": "eOxnUWoDRJWe"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_diacritized_num = 0\n",
        "non_diacirtized_accurate = 0\n",
        "non_diacritized_wrong = defaultdict(lambda: defaultdict(int))\n",
        "diacritized_num = defaultdict(int)\n",
        "diacritized_accurate = defaultdict(int)\n",
        "diacritized_wrong = defaultdict(lambda: defaultdict(int))"
      ],
      "metadata": {
        "id": "GJgSh7WuSPJ6"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for isent, sent in enumerate(testX):\n",
        "  for ilet, let in enumerate(sent):\n",
        "    actual = testY[isent][ilet]\n",
        "    predicted = allY[isent][ilet]\n",
        "    if actual == 0: #no accents\n",
        "      non_diacritized_num += 1\n",
        "      if predicted == 0:\n",
        "        non_diacirtized_accurate += 1\n",
        "      else:\n",
        "        non_diacritized_wrong[let][predicted] += 1 #should not be diacritized but not sure why\n",
        "    else:\n",
        "      base = let\n",
        "      diacr = (actual%100) //10\n",
        "      tone = actual%6\n",
        "      actual_diacr = charToDiacritized[base][diacr][tone]\n",
        "      diacritized_num[ actual_diacr ] += 1\n",
        "      if actual == predicted:\n",
        "        diacritized_accurate[ actual_diacr ] += 1\n",
        "      else:\n",
        "        pred_base = let\n",
        "        pred_diacr = (predicted%100) //10\n",
        "        pred_tone = predicted%6\n",
        "        predicted_diacr = charToDiacritized[pred_base][pred_diacr][pred_tone]\n",
        "        diacritized_wrong[ actual_diacr ] [predicted_diacr]  += 1\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bx0em_yzSSxJ"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pickle results for analysis"
      ],
      "metadata": {
        "id": "js2xdiQrc6qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_diacirtized_accurate/non_diacritized_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTISbW73ceWg",
        "outputId": "3709be7a-a310-4f67-ec9d-43746ac41f08"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9829220370196247"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in diacritized_num:\n",
        "  print(i, diacritized_accurate[i]/diacritized_num[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBtykeCNdKrc",
        "outputId": "9c9e7c65-e027-482a-d9ae-0ce7fcd5eb07"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ể 0.0\n",
            "à 0.7945589803108564\n",
            "ặ 0.0\n",
            "ã 0.4474039931748177\n",
            "ú 0.06709903593339177\n",
            "ấ 0.0\n",
            "ễ 0.0\n",
            "ỡ 0.0\n",
            "ứ 0.0\n",
            "á 0.6301920736546908\n",
            "ữ 0.00045205504222194093\n",
            "ệ 0.003779146117255669\n",
            "ị 0.3855119322555812\n",
            "ò 0.00016073940124573035\n",
            "ừ 0.0\n",
            "ì 0.33807798849350096\n",
            "ự 0.3367738469802778\n",
            "ó 0.5572591530285136\n",
            "ă 0.07033414560870581\n",
            "ẩ 0.0\n",
            "ồ 0.13610768960056308\n",
            "ê 0.0\n",
            "ổ 0.0026729374877523028\n",
            "ạ 0.4026048807238826\n",
            "ô 0.02241390506288015\n",
            "ọ 0.00014334862385321102\n",
            "ầ 0.0\n",
            "í 0.48020854423972464\n",
            "ỗ 0.0\n",
            "ẵ 0.0\n",
            "ẳ 0.0\n",
            "ẻ 0.0\n",
            "ủ 0.8589781085150402\n",
            "ả 0.004609351432880844\n",
            "ư 0.0\n",
            "â 0.0\n",
            "ỏ 0.0\n",
            "ụ 0.0005835918542863286\n",
            "ớ 0.0\n",
            "ộ 0.08777378756996344\n",
            "ũ 0.04026622296173045\n",
            "ử 0.0\n",
            "ẫ 0.0\n",
            "ỉ 0.0\n",
            "ù 0.24868534913133195\n",
            "ờ 0.0\n",
            "ề 0.0\n",
            "ơ 0.0\n",
            "ố 0.08418297051529347\n",
            "ậ 0.0\n",
            "ợ 0.0\n",
            "é 0.00014894250819183795\n",
            "ỹ 0.15812395309882746\n",
            "ý 0.6736834105935101\n",
            "ỷ 0.01793570219966159\n",
            "ẹ 0.0\n",
            "ở 0.0\n",
            "ẽ 0.0\n",
            "ĩ 0.0\n",
            "ế 0.0\n",
            "ỳ 0.0\n",
            "è 0.0\n",
            "õ 0.0\n",
            "ỵ 0.0009775171065493646\n",
            "ằ 0.0\n",
            "ắ 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diacritized_wrong[\"ử\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM1IhGfSdP1r",
        "outputId": "5c42461c-23a7-4d5c-f1d8-b6a76f0f0c6f"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'ự': 19466,\n",
              "             'u': 13279,\n",
              "             'ú': 6536,\n",
              "             'ụ': 624,\n",
              "             'ủ': 2937,\n",
              "             'ù': 2113,\n",
              "             'ũ': 2395,\n",
              "             'ữ': 116})"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#non_diacritized_num = 0\n",
        "#non_diacirtized_accurate = 0\n",
        "#non_diacritized_wrong = defaultdict(lambda: defaultdict(int))\n",
        "#diacritized_num = defaultdict(int)\n",
        "#diacritized_accurate = defaultdict(int)\n",
        "#diacritized_wrong = defaultdict(lambda: defaultdict(int))\n",
        "pickle.dump(non_diacritized_num, open(\"non_diacritized_num.pkl\", \"wb\"))\n",
        "pickle.dump(non_diacirtized_accurate, open(\"non_diacirtized_accurate.pkl\", \"wb\"))\n",
        "pickle.dump(dict(non_diacritized_wrong), open(\"non_diacritized_wrong.pkl\", \"wb\"))\n",
        "\n",
        "pickle.dump(dict(diacritized_num), open(\"diacritized_num.pkl\", \"wb\"))\n",
        "pickle.dump(dict(diacritized_accurate), open(\"diacirtized_accurate.pkl\", \"wb\"))\n",
        "pickle.dump(dict(diacritized_wrong), open(\"diacritized_wrong.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "kYiU1aRKdWIA"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(dict(diacritized_num), open(\"diacritized_num.pkl\", \"wb\"))\n",
        "pickle.dump(dict(diacritized_accurate), open(\"diacirtized_accurate.pkl\", \"wb\"))\n",
        "pickle.dump(dict(diacritized_wrong), open(\"diacritized_wrong.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "-HLWvQMndaHu"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5wdSW98AgNW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.7.4 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "38d37c7b51a76fad05d5106cda319bdfc676f147592bff32468239985737bad5"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}