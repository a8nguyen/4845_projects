{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_F41Ez3cO7xN"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9z2j__fcWeVS"
      },
      "outputs": [],
      "source": [
        "def flat2sent(sents, end = \"<S>\"):\n",
        "  \"\"\"This function turns flat array (1d) of sentences into list of sentences (2d).\n",
        "\n",
        "    End indicates the last token of the sentence.\n",
        "  Args:\n",
        "      sents (): 1d array of sentences\n",
        "      end (str, optional): start of sentence symbol. Defaults to \"<S>\".\n",
        "\n",
        "  Returns:\n",
        "      _type_: 2d array (list of sentences)\n",
        "  \"\"\"\n",
        "  list_of_sent = list()\n",
        "  sent = []\n",
        "  for char in sents:\n",
        "    sent.append(char)\n",
        "    if char[0] == end:\n",
        "      list_of_sent.append(sent)\n",
        "      sent = []\n",
        "  return list_of_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yr5O3vVLRJz7"
      },
      "outputs": [],
      "source": [
        "#train-split for sentences\n",
        "#train_index = int([ind for (ind, token) in enumerate(tokens) if token[0] == \"<S>\"][-1]*0.8)\n",
        "file_train = open('train.tsv')\n",
        "file_test = open('test.tsv')\n",
        "tokens_train = []\n",
        "tokens_test = []\n",
        "for line in file_train:\n",
        "    word = line.strip().split(\"\\t\")\n",
        "    tokens_train.append(tuple(word))\n",
        "for line in file_test:\n",
        "    word = line.strip().split(\"\\t\")\n",
        "    tokens_test.append(tuple(word))\n",
        "sentTrain = flat2sent ( tokens_train )\n",
        "sentTest = flat2sent( tokens_test )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUklCzyc2gei",
        "outputId": "b9ae8ec2-5f98-4365-dc3f-3da68f4e4abe"
      },
      "outputs": [],
      "source": [
        "# map of n preceding words to mutations\n",
        "LAGGED_PRECEDE_MUTATE = defaultdict(lambda: defaultdict(int))\n",
        "def lag_to_create_n_grams(sent: list, ngram=1):\n",
        "    sent = [(\"<IGNORE>\", \"N\") for _ in range(ngram)] + sent #padding in front\n",
        "    for ind, piece in enumerate(sent):\n",
        "        if ind < ngram:\n",
        "            continue\n",
        "        for n in range(1,ngram+1):\n",
        "\n",
        "            prev_phrase = ' '.join( [w[0] for w in sent[ind-n: ind]] )\n",
        "            if \"<IGNORE>\" in prev_phrase:\n",
        "                continue\n",
        "            tag = piece[1]\n",
        "            LAGGED_PRECEDE_MUTATE[prev_phrase][tag] += 1\n",
        "            LAGGED_PRECEDE_MUTATE[prev_phrase][\"occurence\"] += 1\n",
        "            #if (prev_phrase == \"\"):\n",
        "            #    print(ind, n, prev_phrase, tag, piece)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example: The weather is nice with tag D N V A with ngram = 2\n",
        "# {The: {N: 1}, weather: {V: 1}, The weather: {V : 1}, is: {A: 1}, weather is: {A: 1}\n",
        "#lag_to_create_n_grams([(\"The\", \"D\"), (\"weather\", \"N\"), (\"is\", \"V\"), (\"nice\", \"A\")], ngram=2)\n",
        "#creaeting bigram\n",
        "for sent in sentTrain:\n",
        "    lag_to_create_n_grams(sent,2)\n",
        "#LAGGED_PRECEDE_MUTATE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LXJYboMNnjlg"
      },
      "outputs": [],
      "source": [
        "# building the transition matrix\n",
        "def prob_from_count_dict(count_dict,k1,k2):\n",
        "    try:\n",
        "        return count_dict[k1][k2]/count_dict[k1][\"occurence\"]\n",
        "    except ZeroDivisionError:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "TAG_TO_WORD_COUNT =defaultdict(lambda: defaultdict(int))\n",
        "INITIAL_DISTRIBUTION_COUNT = defaultdict(int)\n",
        "#if we were to generate a mutation, how many of it is word w_i\n",
        "def build_emission(sent: list):\n",
        "    for ind, piece in enumerate(sent):\n",
        "\n",
        "        word = piece[0]\n",
        "        tag = piece[1]\n",
        "\n",
        "        TAG_TO_WORD_COUNT[tag][word] += 1\n",
        "        TAG_TO_WORD_COUNT[tag][\"occurence\"] += 1\n",
        "        if ind == 0:\n",
        "            INITIAL_DISTRIBUTION_COUNT[tag] +=1\n",
        "            INITIAL_DISTRIBUTION_COUNT[\"all\"] += 1\n",
        "\n",
        "\n",
        "        #if (prev_phrase == \"\"):\n",
        "        #    print(ind, n, prev_phrase, tag, piece)\n",
        "for sent in sentTrain:\n",
        "    build_emission(sent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NKP5gtMBHnfK"
      },
      "outputs": [],
      "source": [
        "def argmax(Vit_matrix, ind, phrase):\n",
        "    ans = -1\n",
        "    bestTag = None\n",
        "    for t in [\"N\", \"S\", \"U\", \"T\", \"H\"]:\n",
        "        #prob_from_count_dict(count_dict,k1,k2)\n",
        "        temp = Vit_matrix[t][ind-1]*prob_from_count_dict(LAGGED_PRECEDE_MUTATE,phrase,t)\n",
        "        #print(temp)\n",
        "        #print(prob_from_count_dict(LAGGED_PRECEDE_MUTATE, phrase, t))\n",
        "        #print(Vit_matrix[t][ind-1])\n",
        "        if temp > ans:\n",
        "            ans = temp\n",
        "            bestTag = t\n",
        "        #print(t, end=' ')\n",
        "    return ans, bestTag\n",
        "\n",
        "def viterbi(sent):\n",
        "    best_tags = defaultdict(lambda: defaultdict(str))\n",
        "    Vit_matrix = defaultdict(lambda: defaultdict(float)) #V[state][word]\n",
        "    for t in [\"N\",\"S\",\"U\",\"T\",\"H\"]:\n",
        "        # initial probability distribution * emission\n",
        "\n",
        "        start_state_prob = INITIAL_DISTRIBUTION_COUNT[t]/INITIAL_DISTRIBUTION_COUNT[\"all\"]\n",
        "\n",
        "        #prob_from_count_dict(count_dict,k1,k2)\n",
        "        emission = prob_from_count_dict(TAG_TO_WORD_COUNT, t, sent[0][0])\n",
        "\n",
        "        Vit_matrix[t][0] = start_state_prob * emission\n",
        "    for i in range(1, len(sent)):\n",
        "        for t in [\"N\", \"S\", \"U\", \"T\", \"H\"]:\n",
        "            phrase = sent[i-1][0] #TODO: dynamically chose the prhase could be 2 word precede, 1 word precede\n",
        "            val,tag = argmax(Vit_matrix, i, phrase)\n",
        "            emission = prob_from_count_dict(TAG_TO_WORD_COUNT, t,sent[i][0])\n",
        "            Vit_matrix[t][i] = val*emission # transimition_matrix *emission probability\n",
        "            best_tags[t][i] = tag\n",
        "    best_ending = None\n",
        "    best_max = -1\n",
        "    for tag in [\"N\", \"S\", \"U\", \"T\", \"H\"]:\n",
        "        if Vit_matrix[tag][len(sent) - 1] > best_max:\n",
        "            best_max = Vit_matrix[tag][len(sent) - 1]\n",
        "            best_ending = tag\n",
        "    seq = [best_ending]\n",
        "    for i in reversed(range(1,len(sent))):\n",
        "        seq.append(best_tags[seq[-1]][i])\n",
        "    return seq[::-1]\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def eval_scratch(sent):\n",
        "    accuracy = 0\n",
        "    seq = viterbi(sent)\n",
        "    for i in range(len(sent)):\n",
        "        accuracy += int(sent[i][1] == seq[i])\n",
        "    return accuracy\n",
        "#TAG_TO_WORD_COUNT[\"N\"][\"cosaint\"]/TAG_TO_WORD_COUNT[\"N\"][\"occurence\"]\n",
        "#print( prob_from_count_dict(TAG_TO_WORD_COUNT, \"T\", \"cosaint\") )\n",
        "#LAGGED_PRECEDE_MUTATE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#sum(eval_scratch(sent)for sent in sentTrain)/sum(len(sent) for sent in sentTrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.882078"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(eval_scratch(sent)for sent in sentTest) /sum(len(sent) for sent in sentTest)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
